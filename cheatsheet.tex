\documentclass[8pt,a4paper,landscape]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{float}
\usepackage[left=0.75cm,right=0.75cm,top=0.75cm,bottom=1.4cm]{geometry}
\usepackage[skins]{tcolorbox}
\usepackage{multicol}
\usepackage{calrsfs}
\usepackage{xcolor}
\usepackage{csquotes}
\usepackage{mathtools}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{tikz}


\definecolor{DarkGreen}{RGB}{1,100,32}

\setlength{\columnseprule}{1pt}

% Redefine the 'definition' style to have italic text
\newtheoremstyle{definition}      % Name (must match the one you're overriding)
  {\topsep}                       % Space above
  {\topsep}                       % Space below
  {\normalfont}                      % Body font -> italic
  {}                              % Indent amount
  {\bfseries}                     % Theorem head font
  {.}                             % Punctuation after theorem head
  { }                             % Space after theorem head
  {}                              % Theorem head spec
\theoremstyle{definition}
\newtheorem{definition}{Def.}[section]

\newtheoremstyle{example}      % Name of style
  {\topsep}                           % Space above
  {\topsep}                           % Space below
  {\itshape}                       % Body font (non-italic)
  {}                                  % Indent amount
  {\bfseries}                         % Theorem head font
  {.}                                 % Punctuation after theorem head
  { }                                 % Space after theorem head
  {}                                  % Theorem head spec
\theoremstyle{example}
\newtheorem{example}{Ex.}[section]

% Custom theorem style: intuition
\newtheoremstyle{intuition}    % Name
  {\topsep}                       % Space above
  {\topsep}                       % Space below
  {\color{DarkGreen}\normalfont}                   % Body font -> upright
  {}                              % Indent amount
  {\color{black}\bfseries}    % Theorem head font
  {.}                             % Punctuation after theorem head
  { }                             % Space after theorem head
  {}                              % Theorem head spec
\theoremstyle{intuition}
\newtheorem*{intuition}{Int}

\theoremstyle{definition}
\newtheorem{proposition}{Prop}[section]

\DeclareMathAlphabet{\pazocal}{OMS}{zplm}{m}{n}
%\newcommand{\La}{\mathcal{L}}
\newcommand{\Lb}{\pazocal{L}}
%\newcommand{\Fa}{\mathcal{F}}
\newcommand{\Fb}{\pazocal{F}}
\newcommand{\Pb}{\pazocal{P}}
\newcommand{\Ub}{\pazocal{U}}
\newcommand{\Cb}{\pazocal{C}}
\newcommand{\Nb}{\pazocal{N}}

\newcommand{\mydef}[1]{\textcolor{magenta}{\textbf{#1}}}
\newcommand{\prob}[1]{\mathbb{P}\left[ #1 \right]}
\newcommand{\expec}[1]{\mathbb{E}\left[ #1 \right]}
\newcommand{\expect}[1]{\mathbb{E}_{\theta}\left[ #1 \right]}
\newcommand{\biast}[1]{\text{Bias}_{\theta}\left( #1 \right)}
\newcommand{\mset}[1]{\text{MSE}_{\theta}\left( #1 \right)}
\newcommand{\vart}[1]{\text{Var}_{\theta}\left[ #1 \right]}

% TikZ settings for the grid
\newcommand{\squaredpaper}{
  \begin{tikzpicture}[remember picture, overlay]
		\draw[very thin, gray] (current page.south west) grid[step=0.5cm] (current page.north east);  % Draw grid
  \end{tikzpicture}
}

\DeclarePairedDelimiter\abs{\lvert}{\rvert}%
\DeclarePairedDelimiter\norm{\lVert}{\rVert}%

\MakeOuterQuote{"}



\author{Marc-Olivier Jufer \\ \href{mailto:mjufer@ethz.ch}{mjufer@ethz.ch}}
\title{Cheatsheet Probability and Statistics}


\begin{document}
\begin{multicols}{3}
	\maketitle
	
	
	\section{Mathematical framework}
			\subsection{Probability space}
			
			
				%sample space, outcome, elementary experiment
				\begin{definition}
					The set $\Omega$ is called the \mydef{sample space}. An element $\omega \in \Omega$ is called an \mydef{outcome} or \mydef{elementary experiment}.
				\end{definition}
				\begin{example}
					Throw of a die : $\Omega = \{1,2,3,4,5,6\}$
				\end{example}
				
				
				% sigma-algebra
				\begin{definition} \label{dsa}
					A \mydef{sigma-algebra} is a subset $\Fb \subset \Pb (\Omega)$ satisfying the following properties :
					\begin{description}
						\item[P1.] $\Omega \in \Fb$
						\item[P2.] $A \in \Fb \Rightarrow A^c \in \Fb$ : \textcolor{DarkGreen}{If $A$ is an event, "not $A$" is also an event.}
						\item[P3.] $A_1, A_2, \ldots \in \Fb \Rightarrow \bigcup\limits_{i=1}^{\infty}A_i \in \Fb$ : \textcolor{DarkGreen}{if $A_1, A_2, \ldots$ are events, then "$A_1$ or $A_2$ or $\ldots$" is an event}
					\end{description}
				\end{definition}
				\begin{example}
					Examples of sigma-algebras for $\Omega = \{1,2,3,4,5,6\}$ :
					\begin{itemize}
						\item $\Fb = \{\emptyset, \{1,2,3,4,5,6\}\}$
						\item $\Fb = \Pb(\Omega)$
						\item $\Fb = \{\emptyset, \{1,2\}, \{3,4,5,6\}, \{1,2,3,4,5,6\}\}$
					\end{itemize}
					Non examples of sigma-algebras for $\Omega = \{1,2,3,4,5,6\}$ :
					\begin{itemize}
						\item $\Fb  = \{\{1,2,3,4,5,6\}\}$ : \textbf{P2} is not satisfied
						\item $\Fb = \{\emptyset, \{1,2,3\},\{4,5,6\},\{1\},\{2,3,4,5,6\},\Omega\}$ : \textbf{P3} is not satisfied
					\end{itemize}
				\end{example}
				
				
				% probability measure
				\begin{definition} \label{dpm}
					Let $\Omega$ a sample space and $\Fb$ a sigma-algebra. A \mydef{probability measure} on $(\Omega, \Fb)$ is a map
					$$
						\mathbb{P}: \Fb \to \left[0,1\right], \quad A \mapsto \mathbb{P}\left[A\right] 
					$$
					that satisfies the properties
					\begin{description} 
						\item[P1.] $\mathbb{P}[\Omega] = 1$
						\item[P2. (countable additivity)] $\mathbb{P}[A] = \sum_{i=1}^{\infty} \mathbb{P}[A_i]$ if $A = \bigcup_{i=1}^{\infty} A_i$ (disjoint union)
					\end{description}
				\end{definition}
				\begin{intuition}
					A probability measure is a map that associates to each event a number in $[0,1]$
				\end{intuition}
				\begin{example}
					For $\Omega = \{1,2,3,4,5,6\}$ and $\Fb = \Pb(\Omega)$, the mapping $\mathbb{P}: \Fb \to [0,1]$ defined by 
					$$
						\forall A \in \Fb \quad \mathbb{P}[A] = \frac{\abs{A}}{6}
					$$
					is a probability measure on $(\Omega, \Fb)$.
				\end{example}
				
				
				% probability space
				\begin{definition}
					Let $\Omega$ a sample space, $\Fb$ a sigma-algebra and $\mathbb{P}$ a probability measure. The triple $(\Omega, \Fb, \mathbb{P})$ is called a \mydef{probability space}.
				\end{definition}
				
				
				% summary mathematical framework
				\begin{intuition}
					To construct a probabilistic model, we give
					\begin{itemize}
						\item a sample space $\Omega$ : all the possible outcomes of the experiment
						\item a sigma-algebra $\Fb \subset \Pb(\Omega)$ : the set of events
						\item a probability measure $\mathbb{P}$ : gives a number in $[0,1]$ to every event
					\end{itemize}
				\end{intuition}
				
				
				% occurs
				\begin{definition}
					Let $\omega \in \Omega$ (a possible outcome). Let $A$ be an event. We say the event $A$ \mydef{occurs} (\mydef{does not occur}) (for $\omega$) if $\omega \in A$ ($\omega \notin A$).
				\end{definition}
				
				
				
			\subsection{Examples of probability spaces}
			
				\begin{definition}
					Let $\Omega$ be a finite sample space. The \mydef{Laplace model} on $\Omega$ is the triple $(\Omega,\Fb, \mathbb{P})$, where $\Fb = \Pb(\Omega)$ and $\mathbb{P}: \Fb \to \left[0,1\right]$ is defined by
						$$
							\forall A \in \Fb \quad P\left[ A \right] = \frac{\abs{A}}{\abs{\Omega}}
						$$
				\end{definition}
				
				
			\subsection{Properties of Events}
				
				\begin{proposition}
					(Consequences of definition \ref{dsa}). Let $\Fb$ be a sigma-algebra on $\Omega$. We have
					\begin{description} 
						\item[P4.]  $\emptyset \in \Fb$
						\item[P5.] $A_1, A_2, \ldots \in \Fb \Rightarrow \bigcap_{i=1}^{\infty}A_i \in \Fb$
						\item[P6.] $A,B \in \Fb \Rightarrow A \cup B \in \Fb$
						\item[P7.] $A,B \in \Fb \Rightarrow A \cap B \in \Fb$
					\end{description}
				\end{proposition}
				
				\begin{figure}[H]
					\includegraphics[width=\linewidth]{set-operations-representation.png}
					\caption{Representation of set operations}
				\end{figure}
				
%				Other set relations probabilistic interpretation :
%				\begin{itemize}
%					\item $A \subset B$ : If $A$ occurs, then $B$ occurs
%					\item $A \cap B = \emptyset$ : $A$ and $B$ cannot occur at the same time
%					\item $\Omega = A_1 \cup A_2 \cup A_3$ with $A_1,A_2,A_3$ pairwise disjoint : for each outcome $\omega$, one and only on of the events $A_1, A_2, A_3$ is satisfied.
%				\end{itemize}
				
				\begin{figure}[H]
					\includegraphics[width=\linewidth]{set-relations-representation.png}
					\caption{Representation of set relations}
				\end{figure}
				
				
			\subsection{Properties of probability measures}
			
				\begin{proposition}	
					(Consequences of definition \ref{dpm}). Let $\mathbb{P}$ be a probability measure on $(\Omega, \Fb)$.
					\begin{description}
						\item[P3.] We have $\mathbb{P}\left[\emptyset\right] = 0$
						\item[P4.] (\mydef{additivity}) Let $k \geq 1$, let $A_1, \ldots , A_k$ be $k$ pairwise disjoint events, then 
							$$
								\mathbb{P}\left[A_1 \cup \ldots \cup A_k \right] = \prob{A_1} + \ldots + \mathbb{P}\left[ A_k \right]
							$$
						\item[P5.] Let $A$ be an event, then
							$$
								\prob{A^c} = 1 - \prob{A}
							$$
						\item[P6.] If $A$ and $B$ are two events (not necessarily disjoint), then 
							$$
								\prob{A \cup B} = \prob{A} + \prob{B} - \prob{A \cap B}
							$$
					\end{description}
				\end{proposition}
				
				\begin{proposition}
					(\mydef{Monotonicity}). Let $A, B \in \Fb$, then
					$$
						A \subset B \Rightarrow \prob{A} \leq \prob{B}
					$$
				\end{proposition}
				
				\begin{proposition}
					(\mydef{Union bound}).Let $A_1, A_2, \ldots$ be a sequence of events (not necessarily disjoint), then we have
					$$
						\prob{\bigcup\limits_{i=1}^{\infty} A_i} \leq \sum_{i=1}^{\infty}\prob{A_i}
					$$
					Union bound also applies to a finite collection of events.
				\end{proposition}
				
				\begin{proposition}
					Let $(A_n)$ be an increasing sequence of events (i.e. $\forall n \ A_n \subset A_{n+1}$). Then
					\begin{center}
						$\lim_{n \rightarrow \infty} \prob{A_n} = \prob{\bigcup\limits_{n=1}{\infty} A_n}$. \mydef{increasing limit}
					\end{center}
					Let $(B_n)$ be a decreasing sequence of events (i.e. $\forall n \ B_n \supset B_{n+1}$). Then
					\begin{center}
						$\lim_{n \rightarrow \infty} \prob{B_n} = \prob{\bigcap\limits_{n=1}{\infty} B_n}$. \mydef{decreasing limit}
					\end{center}
				\end{proposition}
				
				
			\subsection{Conditional probabilities}
			
				% Conditional probabilitiy
				\begin{definition}
					Let $(\Omega, \Fb, \mathbb{P})$ be some probability space. Let $A, B$ be two events with $\prob{B} > 0$. The \mydef{conditional probability of} $A$ \mydef{given} $B$ is defined by
					$$
						\prob{A \lvert B} = \frac{A \cap B}{B}
					$$
				\end{definition}
				
				\begin{example}
					We consider the probability space $(\Omega, \Fb, \mathbb{P})$ corresponding to the throw of one die. Let $A = \{1,2,3\}$ and $B = \{2,4,6\}$. Then
					$$
						\prob{A \lvert B} = \frac{\prob{A \cap B}}{\prob{B}} = \frac{\frac{1}{6}}{\frac{1}{2}} = \frac{1}{3}
					$$
				\end{example}
				
				\begin{proposition}
					Let $(\Omega, \Fb, \mathbb{P})$ be some probability space. Let $B$ be an event with positive probability. Then $\prob{\ . \ \lvert B}$ is a probability measure on $\Omega$.
				\end{proposition}
				
				% Total probability
				\begin{proposition}
					(\mydef{Formula of total probability}). Let $B_1, \ldots, B_n$ be a partition\footnote{i.e. $\Omega = B_1 \cup \ldots \cup B_n$ and the events are pairwise disjoint.} of the sample space $\Omega$ with $\prob{B_i} > 0$ for every $i \leq i \leq n$. Then one has
					$$
						\forall A \in \Fb \quad \prob{A} = \sum_{i=1}^n \prob{A \lvert B_i} \prob{B_i}
					$$
				\end{proposition}
				
				% Bayes formula
				\begin{proposition}
					(\mydef{Bayes formula}). Let $B_1, \ldots, B_n \in \Fb$ be a partition of $\Omega$ with $\prob{B_i} > 0 \ \forall i$. For every event $A$ with $\prob{A} > 0$ we have 
					$$
						\forall i = 1, \ldots, n \quad \prob{B_i \lvert A} = \frac{\prob{A \vert B_i} \prob{B_i}}{\sum_{j=1}^n \prob{A \lvert B_j} \prob{B_j}}
					$$
				\end{proposition}
				
				\begin{example}
					Test to detect a disease which concerns about $1/10000$ of the population. The test gives the right answer $99\%$ of the time. If a patient has a positive test, what is the probability that he is actually sick ?\\
					We modeled the situation as $\Omega = \{0,1\} \times \{0,1\}$. $\Fb = \Pb(\Omega)$ and an outcome is $\omega = (\omega_1, \omega_2)$, where $\omega_1$ is $1$ if the patient is sick and $\omega_2$ is $1$ if the test is positive. Let $S = \{(1,0), (1,1)\}$ be the event that the patient is sick and $T = \{(0,1),(1,1)\}$ the event that the test is positive.\\
					From the hypotheses, we have
					$$
						\prob{S} = \frac{1}{10000}, \quad \prob{T \lvert S} = \frac{99}{100}, \quad \prob{T \lvert S^c} = \frac{1}{100}
					$$
					By applying the Bayes formula to the partition $\Omega = S \cup S^c$, we obtain
					$$
						\prob{S \lvert T} = \frac{\prob{T \lvert S} \prob{S}}{\prob{T \lvert S} \prob{S} + \prob{T \lvert S^c} \prob{S^c}} \simeq 0.0098
					$$
				\end{example}
				
				
			\subsection{Independence}
			
				% Independence of events
				\begin{definition}
					Let $(\Omega, \Fb, \mathbb{P})$ be a probability space. Two events $A$ and $B$ are said to be \mydef{independent} if 
					$$
						\prob{A \cap B} = \prob{A} \prob{B}
					$$
					$A$ is independent of $B$ iff $A$ is independent of $B^c$. \\
					If $\prob{A} \in \{0,1\}$, then $A$ is independent of every event. \\
					If $A$ is independent with itself (i.e. $\prob{A \cap A} = \prob{A}^2$), then $\prob{A} \in \{0, 1\}$.
				\end{definition}
				
				\begin{proposition}
					Let $A, B \in \Fb$ be two events with $\prob{A}, \prob{B} > 0$. Then the following are equivalent :
					\begin{enumerate}[label=\roman*.]
						\item $\prob{A \cap B} = \prob{A} \prob{B}$ : \textcolor{DarkGreen}{A and B are independent}
						\item $\prob{A \lvert B} = \prob{A}$ : \textcolor{DarkGreen}{the occurrence of B has no influence on A}
						\item $\prob{B \lvert A} = \prob{B}$ : \textcolor{DarkGreen}{the occurrence of A has no influence on B}
					\end{enumerate} 
				\end{proposition}
				
				% Independence of collection of events
				\begin{definition}
					Let $I$ be an arbitrary set of indices. A collection of events $(A_i)_{i \in I}$ is said to be \mydef{independent} if 
					$$
						\forall J \subset I finite \quad \prob{\bigcap\limits_{j \in J} A_j} = \prod_{j \in J} \prob{A_j}
					$$
				\end{definition}
				
				\begin{intuition}
					Three events $A$, $B$ and $C$ are independent if the following 4 equations are satisfied :
						$$\prob{A \cap B} = \prob{A} \prob{B} $$
						$$\prob{A \cap C} = \prob{A} \prob{C} $$
						$$\prob{B \cap C} = \prob{B} \prob{C} $$
						$$\prob{A \cap B \cap C} = \prob{A}\prob{B}\prob{C}$$
				\end{intuition}


	\newpage
	\section{Random variables and distribution functions}
			\subsection{Abstract definition}
			
				% Random variable
				\begin{definition}
					Let $(\Omega, \Fb, \mathbb{P})$ be a probability space. A \mydef{random variable} (r.v.) is a map $X : \Omega \to \mathbb{R}$ s.t.
					$$						
						\forall a \in \mathbb{R} \quad \{\omega \in \Omega : X(\omega) \leq a\} \in \Fb
					$$
				\end{definition}
				
				\begin{example}
					We throw a fair die. The sample space is $\Omega = \{1,2,3,4,5,6\}$ and we consider the Laplace model $(\Omega, \Fb, \mathbb{P})$. Suppose we gamble on the outcome in such a way that our profit is $-1$ if the outcome is $1$, $2$ or $3$ ; $0$ if the outcome is $4$ and $2$ if the outcome is $5$ or $6$. Our profit can be represented by the mapping $X$ defined by 
					$$
						\forall \omega \in \Omega \quad X(\omega) = \begin{cases}
							-1 \quad \text{if } \omega = 1,2,3, \\
							0 \quad \text{if } \omega = 4, \\
							2 \quad \text{if } \omega = 5,6
						\end{cases}
					$$
					Since $\Fb = \Pb(\Omega)$, we have $\{\omega : X(\omega) \leq a\} \in \Fb$ for every $a$. Therefore, $X$ is a random variable on $(\Omega, \Fb, \mathbb{P})$.
				\end{example}
				
				\begin{definition}
					When events are defined in terms of random variable, we omit the dependence in $\omega$. E.g. for $a \leq b$ we write 
					$$\{X \leq a\} = \{\omega \in \Omega : X(\omega) \leq a\}$$
					$$\{a < X \leq b\} = \{\omega \in \Omega : a < X(\omega) < b\}$$
					$$\{X \in \mathbb{Z}\} = \{\omega \in \Omega : X(\omega) \in \mathbb{Z}\}$$
					When consider the probability of events as above, we omit the brackets 
					$$ \prob{X \leq a} = \prob{\{X \leq a\}} = \prob{\{\omega \in \Omega : X(\omega) \leq a\}}$$
				\end{definition}
				
			
			\subsection{Distribution function}
			
				% Distribution function
				\begin{definition}
					Let $X$ be a random variable on a prob. space $(\Omega, \Fb, \mathbb{P})$. The \mydef{distribution function of X} is the function $F_X : \mathbb{R} \to \left[0,1\right]$ defined by
					$$
						\forall a \in \mathbb{R} \quad F_X(a) = \prob{X \leq a}
					$$
				\end{definition}
				
				\begin{example}
					Same example with the die. Let $X$ be the random variable defined as above. For $a \in \mathbb{R}$ we have 
					$$
						F_X(a) = \begin{cases}
							0 \quad \text{if } a < -1,\\
							1/2 \quad \text{if } -1 \leq a < 0, \\
							2/3 \quad \text{if } 0 \leq a < 2, \\
							1 \quad \text{if } a \geq 2
						\end{cases}
					$$
					\begin{figure}[H]
						\includegraphics[width=\linewidth]{distribution_function.png}
						\caption{Graph of the distribution function $F_X$}
					\end{figure}
				 \end{example}
				 
				 \begin{proposition}
				 	(\mydef{Basic identity}). Let $a < b$ be two real numbers. Then 
				 	$$
				 		\prob{a < X \leq b} = F(b) - F(a)
				 	$$
				 \end{proposition}
				 
				 \begin{proposition} \label{FX-conditions}
				 	Let $X$ be a r.v. on some probability space $(\Omega, \Fb, \mathbb{P})$. The distribution function $F = F_X : \mathbb{R} \to \left[0,1\right]$ of $X$ satisfies the following properties :
					\begin{enumerate}[label=\roman*.] \label{F_3points}
				 		\item $F$ is nondecreasing
				 		\item $F$ is right continuous\footnote{i.e. $F(a) = \lim_{h \downarrow 0}F(a+h)$ for every $a \in \mathbb{R}$}
				 		\item $\lim\limits_{a \rightarrow -\infty} F(a) = 0$ and $\lim\limits_{a \rightarrow \infty} F(a) = 1$
				 	\end{enumerate}
				 \end{proposition}
				 
				 
			\subsection{Independence}
				
				% Independence of random variables
				\begin{definition}
					Let $X_1, \ldots, X_n$ be $n$ random variables on some probability space $(\Omega, \Fb, \mathbb{P})$. We say that they are \mydef{independent} if
					$
						\forall x_1, \ldots, x_n \in \mathbb{R} \quad \prob{X_1 \leq x_1, \ldots, X_n \leq x_n} 
						= \prob{X_1 \leq x_1} \ldots \prob{X_n \leq x_n}
					$.\\
					One can show that $X_1, \ldots, X_n$ are independent iff $\forall I_1 \subset \mathbb{R}, \ldots, I_n \subset \mathbb{R}$ intervals $\{X_1 \in I_1\}, \ldots, \{X_n \in I_n\}$ are independent.
				\end{definition}
				
				% Grouping
				\begin{proposition}
					(\mydef{Grouping}). Let $X_1, \ldots, X_n$ be $n$ independent r.v. Let $1 \leq i_1 < i_2 < \ldots < i_k \leq n$ be some indices and $\phi_1, \ldots, \phi_k$ some functions. Then 
					$Y_1 = \phi_1 (X_1, \ldots, X_{i_1}), Y_2 = \phi_2(X_{i_1 + 1}, \ldots, X_{i_2}), \ldots, Y_k = \phi_k (X_{i_{k-1}+1}, \ldots, X_{i_k})$
					are independent.
				\end{proposition}
				
				% Sequences of i.i.d. random variables
				\begin{definition}
					An infinite sequence $X_1, X_2, \ldots$ of random variables is said to be 
					\begin{itemize}
						\item \mydef{independent} if $X_1, \ldots, X_n$ are independent for every $n$
						\item \mydef{independent and identically distributed} (i.i.d) if they are independent and they have the same distribution function, i.e. $\forall i,j \quad F_{X_i} = F_{X_j}$.
					\end{itemize}
				\end{definition}
				
				
			\subsection{Transformation of random variables}
				
				% peut-être commenter les deux premières lignes
				We can create r.v. from other r.v. on the same probability space. For example, consider $Z_1 = \exp(X_1), Z_2 = X_1 + X_2$. Not to forget : r.v. are maps $\Omega \to \mathbb{R}$.\\
				We can work with r.v. as if they were real numbers with the following notation :
				\begin{definition}
					If $X$ is a r.v. and $\phi : \mathbb{R} \to \mathbb{R}$, then we write 
					$$
						\phi(X) := \phi \circ X 
					$$
					to to $\phi(X)$ a new mapping $\Omega \to \mathbb{R}$.\\
					We also consider function of several variables. If $X_1, \ldots, X_n$ are $n$ r.v. and $\phi : \mathbb{R}^n \to \mathbb{R}$, then we write 
					$$
						\phi(X_1, \ldots, X_n) := \phi \circ (X_1, \ldots, X_n)
					$$
				\end{definition} 

			
			\subsection{Construction of random variables}
				
				% Bernoulli random variables
			\begin{definition} \label{Bernoulli_rv}
					Let $p \in \left[0,1\right]$. A r.v. $X$ is said to be a \mydef{Bernoulli r.v. with parameter $p$} if 
					$$
					\prob{X = 0} = 1-p \quad \text{and} \quad \prob{X = 1} = p
					$$
				In this case, we write $X \sim \text{Ber}(p)$.
				\end{definition}

				% Existence theorem of Kolmogorov
				\begin{proposition}
					(\mydef{Existence theorem of Kolmogorov}). There exists a probability space $(\Omega, \Fb, \mathbb{P})$ and an infinite sequence of r.v. $X_1, X_2, \ldots$ (on this probability space) that is an iid sequence of Bernoulli r.v. with parameter $1/2$.
				\end{proposition}

				% Uniform random variable
				\begin{proposition}
					A r.v. $U$ is said to be \mydef{uniform r.v. in $\left[0,1\right]$} if its distribution function is equal to 
					$$
					F_U(x) = \begin{cases}
						0 \quad x < 0 \\
						x \quad 0 \leq x \leq 1 \\
						1 \quad x > 1
					\end{cases}
					$$
					In this csae, we write $U \sim \Ub (\left[0,1\right])$.
				\end{proposition}

				% Distribution functions of benroulli and uniform random variables
				\begin{figure}[H] 
					\includegraphics[width=\linewidth]{bernoulli_uniform_rv.png}
					\caption{Left: distribution function of a Bernoulli r.v. with parameter $p$. Right: distribution function of a uniform r.v. in $\left[0,1\right]$.}
				\end{figure}

				\begin{proposition}
					The mapping $Y : \Omega \to \left[0,1\right]$ defined by $Y(\omega) = \sum_{n=1}^{\infty}2^{-n}X_n(\omega)$ is a uniform r.v. in $\left[0,1\right]$.
				\end{proposition}

				% Generalized inverse
				\begin{definition}
					The \mydef{generalized inverse} of $F$\footnote{\label{note3}satifying prop. \ref{F_3points}} is the mapping $F^{-1} : (0,1) \to \mathbb{R}$ defined by
					$$
						\forall \alpha \in (0,1) \quad F^{-1}(\alpha) = \text{inf}\{x \in \mathbb{R} : F(x) \geq \alpha\}
					$$
				\end{definition}

				\begin{intuition}
					By definition of the infimum and using right continuity of $F$, we have $\forall x \in \mathbb{R}$ and $\forall \alpha \in (0,1)$
					$$
						(F^{-1}(\alpha) \leq x) \Longleftrightarrow (\alpha \leq F(x))
					$$
				\end{intuition}

				% Inverse transform sampling
				\begin{proposition}
					(\mydef{Inverse transform sampling}). Let $F : \mathbb{R} \to \left[0,1\right]$.\footnote{See footnote \ref{note3}} Let $U$ be a uniform r.v. in $\left[0,1\right]$. Then the r.v. $X = F^{-1}(U)$ has distribution $F_X = F$.
				\end{proposition}

				% General sequence of independent random variables
				\begin{proposition}
					Let $F_1, F_2, \ldots$ be a sequence of functions $\mathbb{R} \to \left[0,1\right]$.\footnote{See footnote \ref{note3}} Then there exist a probability space $(\Omega, \Fb, \mathbb{P})$ and a sequence of independent r.v. $X_1, X_2, \ldots$ on this probability space s.t.
					\begin{itemize}
						\item for every $i$ $X_i$ has distribution function $F_i$ (i.e. $\forall x \prob{X_1 \leq x} = F_i(x)$)
						\item $X_1, X_2, \ldots$ are independent
					\end{itemize}
				\end{proposition}


	\section{Discrete and continuous r.v.}
		\subsection{Discontinuity / continuity points of $F$}
			
			% Probability of a given value
			\begin{proposition}
				Let $X : \Omega \to \mathbb{R}$ be a r.v. with distribution function $F$. Then for every $a$ in $\mathbb{R}$ we have
				$$
					\prob{X = a} = F(a) - F(a-)
				$$
				where $F(a-) := \lim_{h \downarrow 0}F(a-h)$.
			\end{proposition}

			\begin{intuition}
				Fix $a \in \mathbb{R}$
				\begin{itemize}[label=$\rightarrow$]
					\item If $F$ is not continuous at a point $a \in \mathbb{R}$, then the "jump size" $F(a) - F(a-)$ is equal to the probability that $X = a$
					\item If $F$ is continuous at a point $a \in \mathbb{R}$, then $\prob{X = a} = 0$
				\end{itemize}
			\end{intuition}

	
		\subsection{Almost sure events}
			
			% almost surely
			\begin{definition}
				Let $A \in \Fb$ be an event. We say that $A$ occurs \mydef{almost surely} (\mydef{a.s.}) if $\prob{A} = 1$.
			\end{definition}

		
		\subsection{Discrete random variables}

			% Discrete random variables
			\begin{definition}
				A r.v. $X : \Omega \to \mathbb{R}$ is said to be \mydef{discrete} if there exists some set $W \subset \mathbb{R}$ finite or countable s.t. $X \in W \quad \text{a.s.}$.
			\end{definition}

			% Distribution of discrete random variables
			\begin{definition}
				Let X be a discrete r.v. taking some values in some finite or countable set $W \subset \mathbb{R}$. The \mydef{distribution of} $X$ is the sequence of numbers $(p(x))_{x \in W}$ defined by 
				$$
					\forall x \in W \quad p(x) := \prob{X = x}
				$$
			\end{definition}

			\begin{proposition}
				The distribution $(p(x))_{x \in W}$ of a discrete r.v. satisfies $\sum_{x \in W} p(x) = 1$.
			\end{proposition}

			\begin{proposition}
				Let $X$ be a discrete r.v. with values in a finite or countable set $W$ almost surely, and distribution $p$. Then the distribution function of $X$ is given by
				$$
				\forall x \in \mathbb{R} \quad F_X(x) = \sum_{\substack{y \leq x \\ y \in W}} p(y)
				$$
			\end{proposition}

			\begin{intuition}
				$W = \{\text{positions of the jumps of } F_X\}$,\\
				$p(x) = \text{"height of the jump" at } x \in W$.
			\end{intuition}


		\subsection{Examples of discrete random variables}
			
		 	The simplest (non constant) r.v. is the Bernoulli r.v. defined in definition \ref{Bernoulli_rv}.

			% Binomial distribution
			\begin{definition}
				Let $0 \leq p \leq 1$, let $n \in \mathbb{N}$. A r.v. $X$ is said to be a \mydef{binomial r.v. with parameters $n$ and $p$} if it takes values in $W = \{0,\ldots, n\}$ and 
				$$
				\forall k \in \{0,\ldots,n\} \quad \prob{X = k} = \binom{n}{k} p^k (1-p)^{n-k}
				$$
				In that case we write $X \sim \text{Bin}(n,p)$.
			\end{definition}

			% Sum of independent Bernoulli and binomial
			\begin{proposition}
				(\mydef{Sum of independent Bernoulli and binomail}. Let $0 \leq p \leq 1$, let $n \in \mathbb{N}$. Let $X_1, \ldots, X_n$ be $n$ independent Bernoulli r.v. with parameter $p$. Then
				$$
					S_n := X_1 + \ldots + X_n
				$$
				is a binomial r.v. with parameter $n$ and $p$.
			\end{proposition}

			\begin{intuition}
				In particular, the distribution $\text{Bin}(1,p)$ is the same as the distribution $\text{Ber}(p)$. On can also check that if $X \sim \text{Bin}(m,p)$ and $Y \sim \text{Bin}(n,p)$ and $X,Y$ are independent, then $X + Y \sim \text{Bin}(m + n, p)$.
			\end{intuition}

			% Geometric distribution
			\begin{definition}
				Let $0 \leq p \leq 1$. A r.v. $X$ is said to be a \mydef{geometric r.v. with parameter $p$} if it takes values in $W = \mathbb{N} \setminus \{0\}$ and 
				$$
					\forall k \in \mathbb{N} \quad \prob{X = k} = (1-p)^{k-1} \cdot p
				$$
				In this case, we write $X \sim \text{Geom}(p)$.
			\end{definition}

			% Geometric r.v. appears naturally as the first success in an infinite sequence of Bernoulli experiments with parameter p
			\begin{proposition}
				Let $X_1, X_2, \ldots$ be a sequence of infinitely many independent Bernoulli r.v. with parameter $p$. Then
				$$
				T := \min\{n \geq 1 : X_n = 1\}
				$$
				is a geometric r.v. with parameter $p$.
			\end{proposition}

			% Absence of memory of the geometric distribution
			\begin{proposition}
				(\mydef{Absence of memory of the geometric distribution}). Let $T \sim \text{Geom}(p)$ for some $0 < p < 1$. Then
				$$
					\forall n \geq 0 \ \forall k \geq 1 \quad \prob{T \geq n + k \lvert T > n} = \prob{T \geq k}
				$$
			\end{proposition}

			% Poisson distribution
			\begin{definition}
				Let $\lambda > 0$ be a positive real number. A r.v. $X$ is said to be a \mydef{Poisson r.v. with parameter $\lambda$} if it takes values in $W = \mathbb{N}$ and 
				$$
				\forall k \in \mathbb{N} \quad \prob{X = k} = \frac{\lambda^k}{k!} e^{-\lambda}
				$$
				In this case, we write $X \sim \text{Poisson}(\lambda)$.
			\end{definition}

			% Poisson distribution appears naturally as an approximation of a binomail distribution when the parameter n is large and the parameter p is small
			\begin{proposition}
				(\mydef{Poisson approximation of the binomail}). Let $\lambda > 0$. For every $n \geq 1$, consider a r.v. $X_n \sim \text{Bin}(n, \frac{\lambda}{n})$. Then
				$$
					\forall k \in \mathbb{N} \quad \lim\limits_{n \to \infty} \prob{X_n = k} = \prob{N = k}
				$$
				where $N$ is a Poisson r.v. with parameter $\lambda$.
			\end{proposition}


		\subsection{Continous random variables}
			
			% Continuous random variables
			\begin{definition}
				A r.v. $X : \Omega \to \mathbb{R}$ is said to be \mydef{continuous} if its distribution function $F_X$ can be written as
				$$
				F_X(a) = \int_{-\infty}^{a} f(x)\, dx \quad \text{for all } a \in \mathbb{R}
				$$
				for some nonnegative function $f : \mathbb{R} \to \mathbb{R_+}$, called the \mydef{density} of $X$.
			\end{definition}

			\begin{intuition}
				$f(x)\, dx$ represents the probability that $X$ takes a value in the infinitesimal interval $\left[x, x + dx\right]$.
			\end{intuition}

			\begin{proposition}
				The density $f$ of a r.v. satisfies $\int_{-\infty}^{+\infty} f(x)\, dx = 1$.
			\end{proposition}

			\begin{proposition}
				Let $X$ be a r.v. Assume the distribution function $F_X$ is continuous and piecewise $\Cb^1$, i.e. that there exist $x_0 = -\infty < x_1 < \ldots < x_{n-1} < x_n = +\infty$ s.t. $F_X$ is $\Cb^1$ on every interval $(x_i, x_{i+1}$. Then $X$ is a continuous r.v. and a density $f$ can be constructed by defining 
				$$
					\forall x \in (x_i, x_{i+1}) \quad f(x) = F'_X(x)
				$$
				and setting arbitrary values at $x_1, \ldots, x_{n-1}$.
			\end{proposition}


		\subsection{Examples of continuous random variables}
				
			% Uniform distribution in [a,b], a<b
			\begin{definition}
				A continuous r.v. $X$ is said to be \mydef{uniform in $\left[a,b\right]$} if its density is equal to 
				$$
					f_{a,b}(x) = \begin{cases}
						\frac{1}{b-a} \quad x \in \left[a,b\right], \\
						0 						\quad x \notin \left[a,b\right]
					\end{cases}
				$$
				In this case, we write $X \sim \Ub(\left[a,b\right])$.
			\end{definition}

			% Exponential distribution with \lambda > 0
			\begin{definition}
				A continuous r.v. $T$ is said to be \mydef{exponential with parameter $\lambda > 0$} if its density is equal to 
				$$
					f_\lambda(x) = \begin{cases}
						\lambda e^{-\lambda x} \quad x \geq 0, \\
						0 										 \quad x < 0
					\end{cases}
				$$
				In this case, we write $T \sim \text{Exp}(\lambda)$.
			\end{definition}

			% Normal distribution with parameters m and \sigma^2 > 0
			\begin{definition}
				A continuous r.v. $X$ is said to be \mydef{normal with parameters $m$ and $\sigma^2 > 0$} if its density is equal to 
				$$
					f_{m,\sigma}(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} e^{\frac{(x-m)^2}{2 \sigma^2}}
				$$
				In this case, we write $X \sim \Nb(m,\sigma^2)$.
			\end{definition}


	\section{Expectation}
		\subsection{Expectation for general r.v.}
			\begin{definition}
				Let $X : \Omega \to \mathbb{R}_+$ be a r.v. with nonnegative values. The \mydef{expectation} of $X$ is defined as 
				$$
					\expec{X} = \int_0^\infty (1 - F_X(x)) dx
				$$
			\end{definition}

			\begin{proposition}
				Let $X$ be a nonnegative r.v. Then we have $\expec{X} \geq 0$, with equality iff $X=0$ almost surely.
			\end{proposition}

			\begin{definition}
				Let $X$ be a r.v. If $\expec{\lvert X \lvert} < \infty$, then the expectation of $X$ is defined by $\expec{X} = \expec{X_+} - \expec{X_-}$, where $X_+$ and $X_-$ are the positive and negative parts of $X$ defined by $X_+(\omega) = \begin{cases} X(\omega) \quad \text{if } X(\omega) \geq 0, \\ 0 \quad \text{if } X(\omega) < 0,\end{cases}$ and $X_-(\omega) = \begin{cases} -X(\omega) \quad \text{if } X(\omega) \leq 0, \\ 0 \quad \text{if } X(\omega) > 0.\end{cases}$
			\end{definition}


		\subsection{Expectation of a discrete r.v.}
			\begin{proposition}
				Let $X : \Omega \to \mathbb{R}$ be a discrete r.v. with values in $W$ (finite or countable) almost surely. We have
				$$
					\expec{X} = \sum_{x \in W} x \cdot \prob{X = x}
				$$
				provided the sum is well defined.
			\end{proposition}

			\begin{proposition}
				Let $X : \Omega \to \mathbb{R}$ be a discrete r.v. with values in $W$ (finite or countable) almost surely. For every $\phi : \mathbb{R} \to \mathbb{R}$, we have
				$$
					\expec{\phi(X)} = \sum_{x \in W} \phi(x) \cdot \prob{X = x}
				$$
				provided the sum is well defined.
			\end{proposition}


		\subsection{Expectation of a continuous r.v.}
			\begin{proposition}
				Let $X$ be a contiuous r.v. with density $f$. Then we have 
				$$
					\expec{X} = \int_{-\infty}^\infty x \cdot f(x) dx
				$$
				provided the integral is well defined.
			\end{proposition}

			\begin{proposition}
				Let $X$ be a continuous r.v. with density $f$. Let $\phi : \mathbb{R} \to \mathbb{R}$ be s.t. $\phi(X)$ is a r.v. Then we have
				$$
					\expec{\phi(X)} = \int_{-\infty}^\infty \phi(x) f(x) dx
				$$
				provided the integral is well defined.
			\end{proposition}


		\subsection{Calculus}
			\begin{proposition}
				(\mydef{Linearity of the expectation}). Let $X,Y : \Omega \to \mathbb{R}$ be r.v.'s, let $\lambda \in \mathbb{R}$. Provided the expectations are well defined, we have 
				\begin{enumerate}
					\item $\expec{\lambda \cdot X} = \lambda \cdot \expec{X}$
					\item $\expec{X + Y} = \expec{X} + \expec{Y}$
				\end{enumerate}
			\end{proposition}

			\begin{proposition}
				Let $X,Y$ be two r.v. If $X$ and $Y$ are independent, then $\expec{XY} = \expec{X} \expec{Y}$.
			\end{proposition}


		\subsection{Tailsum formulas}
			\begin{proposition}
				(\mydef{Tailsum formula for nonnegative r.v.'s}). Let $X$ be a r.v., s.t. $X \geq 0$ almost surely. Then we have $\expec{X} = \int_0^\infty \prob{X > x} dx$.
			\end{proposition}

			\begin{proposition}
				(\mydef{Tailsum formula for discrete r.v.'s}). Let $X$ be a discrete r.v. taking values in $\mathbb{N} = \{0,1,2,\ldots\}$. Then
				$
					\expec{X} = \sum_{n=1}^\infty \prob{X \geq n}
				$.
			\end{proposition}


		\subsection{Characterizations via expecations}
			\begin{proposition}
				Let $X$ be a r.v. Let $f : \mathbb{R} \to \mathbb{R}$ s.t. $\int_{-\infty}^{+\infty} f(x) dx = 1$. then the following are equivalent :
				\begin{enumerate}[label=\roman*.]
					\item $X$ is continuous with density $f$,
					\item For every function $\phi : \mathbb{R} \to \mathbb{R}$ piecwise continuous, bounded : $\expec{\phi(X)} = \int_{-\infty}^\infty \phi(x) f(x) dx$
				\end{enumerate}
			\end{proposition}

			\begin{proposition}
				Let $X,Y$ be two discrete r.v.'s. Then the following are equivalent 
				\begin{enumerate}[label=\roman*.]
					\item $X,Y$ are independent
					\item For every $\phi : \mathbb{R} \to \mathbb{R}$, $\psi : \mathbb{R} \to \mathbb{R}$ piecewise contiuous, bounded : $\expec{\phi(X)\psi(Y)} = \expec{\phi(X)} \expec{\psi(Y)}$.
				\end{enumerate}
			\end{proposition}

			\begin{proposition}
				Let $X_1, \ldots, X_n$ be $n$ r.v.'s. Then the following are equivalent
				\begin{enumerate}[label=\roman*.]
					\item $X_1, \ldots, X_n$ are independent
					\item For every $\phi_1 : \mathbb{R} \to \mathbb{R}, \ldots, \phi_n : \mathbb{R} \to \mathbb{R}$ piecewise continuous, bounded : $\expec{\phi_1(X_1) \cdots \phi_n(X_n)} = \expec{\phi_1(X_1)} \cdots \expec{\phi_n(X_n)}$.
				\end{enumerate}
			\end{proposition}


		\subsection{Inequalities}
			% Monotonicity
			\begin{proposition}
				(\mydef{Monotonicity}). Let $X,Y$ be two r.v.'s s.t. $X \leq Y$ a.s. Then
				$
					\expec{X} \leq \expec{Y}
				$,
				provided the two expectations are well defined.
			\end{proposition}

			% Markov's inequality
			\begin{proposition}
				(\mydef{Markov's inequality}). Let $X$ be a nonnegative r.v. Then for every $a > 0$, we have 
				$$
					\prob{X \geq a} \leq \frac{\expec{X}}{a}
				$$
			\end{proposition}

			% Jensen's inequality
			\begin{proposition}
				(\mydef{Jensen's inequality}). Let $X$ be a r.v. Let $\phi : \mathbb{R} \to \mathbb{R}$ be a convex function. If $\expec{\phi(X)}$ and $\expec{X}$ are well defined, then
				$$
					\phi(\expec{X}) \leq \expec{\phi(X)}
				$$
			\end{proposition}


		\subsection{Variance}
			% Variance of X
			\begin{definition}
				Let $X$ be a variable s.t. $\expec{X^2} < \infty$. The \mydef{variance of $X$} is defined by 
				$$
					\sigma^2_X = \expec{(X - m)^2}, \quad \text{where } m = \expec{X}
				$$
				The square root $\sigma_X$ of the variance is called the \mydef{standard deviation of $X$}.
			\end{definition}

			\begin{proposition}
				Let $X$ be a r.v. s.t. $\expec{X^2} < \infty$. Then for every $a \geq 0$ we have 
				$$
					\prob{\lvert X - m\lvert \geq a} \leq \frac{\sigma^2_X}{a^2}, \quad \text{where } m = \expec{X}
				$$
			\end{proposition}

			\begin{proposition}
				(\mydef{Basic properties of the variance}).
				\begin{enumerate}
					\item Let $X$ be a r.v. with $\expec{X^2} < \infty$. Then $\sigma^2_X = \expec{X^2} - \expec{X}^2$.
					\item Let $X$ be a r.v. with $\expec{X^2} < \infty$, let $\lambda \in \mathbb{R}$. Then $\sigma^2_{\lambda X} = \lambda^2 \cdot \sigma^2_X$.
					\item Let $X_1, \ldots, X_n$ be $n$ pairwise independent r.v.'s and $S = X_1 + \ldots + X_n$. Then $\sigma^2_S = \sigma^2_{X_1} + \ldots + \sigma^2_{X_n}$.
				\end{enumerate}
			\end{proposition}


		\subsection{Covariance}
			% Covariance
			\begin{definition}
				Let $X,Y$ be two r.v.'s. Assume that $\expec{X^2} < \infty$ and $\expec{Y^2} < \infty$ (finite second moment). We define the \mydef{covariance between $X$ and $Y$} as 
				$$
					\text{Cov}(X,Y) = \expec{XY} - \expec{X}\expec{Y}.
				$$
			\end{definition}

			\begin{intuition}
				With $X$ and $Y$ independent : $\text{Cov}(X,Y) = 0$.
			\end{intuition}

	
	\section{Joint distribution}
		\subsection{Discrete joint distributions}
			\begin{definition}
				Let $X_1, \ldots, X_n$ be $n$ discrete r.v.'s with $X_i \in W_i$ almost surely, for some $W_i \subset \mathbb{R}$ finite or countable. The \mydef{joint distribution} of $(X_1, \ldots, X_n)$ is the collection $p = (p(x_1, \ldots, x_n))_{x_1 \in W_1, \ldots, x_n \in W_n}$ defined by
				$$
					p(x_1, \ldots, x_n) = \prob{X_1 = x_1, \ldots, X_n = x_n}
				$$
			\end{definition}

			\begin{proposition}
				The joint distribution of some r.v.'s $X_1, \ldots, X_n$ satisfies 
				$
					\sum_{x_1 \in W_1, \ldots, x_n \in W_n} p(x_1, \ldots, x_n) = 1
				$.
			\end{proposition}

			\begin{proposition}
				Let $n \geq 1$ and $\phi : \mathbb{R}^n \to \mathbb{R}$ be an arbitrary function. Let $X_1, \ldots, X_n$ be $n$ discrete r.v.'s on $(\Omega, \Fb, \mathbb{P})$ with respective values in some finite or countable sets $W_1, \ldots, W_n$ a.s. Then $Z = \phi(X_1, \ldots, X_n)$ is a discrete r.v. with values in $W = \phi(W_1 \times \ldots \times W_n)$ a.s. and with distribution given by
				$$
				\forall z \in W \quad \prob{Z = z} = \sum\limits_{\substack{x_1 \in W, \ldots, x_n \in W_n\\ \phi(x_1, \ldots, x_n) = z}} \prob{X_1 = x_1, \ldots, X_n = x_n}
				$$
			\end{proposition}

			% Marginal distributions
			\begin{proposition}
				(\mydef{Marginal distributions}). Let $X_1, \ldots, X_n$ be $n$ discrete r.v.'s with joint distribution $p = (p(x_1, \ldots, x_n))_{x_1 \in W_1, \ldots, x_n \in W_n}$. For every $i$, we have $\forall z \in W_i
					\prob{X_1 = z} = \sum\limits_{x_1, \ldots, x_{i-1}, x_{i+1}, \ldots, x_n} p(x_1, \ldots, x_{i-1}, z, x_{i+1}, \ldots, x_n)
				$
			\end{proposition}

			% Expectation of the image
			\begin{proposition}
				(\mydef{Expectation of the image}).
				Let $X_1, \ldots$, $X_n$ be $n$ discrete r.v.'s with joint distribution $p = (p(x_1, \ldots, x_n))_{x_1 \in W_1, \ldots, x_n \in W_n}$. Let $\phi : \mathbb{R}^n \to \mathbb{R}$, then
				$$
					\expec{\phi(X_1, \ldots, X_n)} = \sum_{x_1, \ldots, x_n} \phi(x_1, \ldots, x_n)p(x_1, \ldots, x_n)
				$$
				whenever the sum is well-defined.
			\end{proposition}

			% Independence
			\begin{proposition}
				(\mydef{Independence}).
				Let $X_1, \ldots, X_n$ be $n$ discrete r.v.'s with joint distribution $p = p(x_1, \ldots, x_n))_{x_1 \in W_1, \ldots, x_n \in W_n}$. The following are equivalent
				\begin{enumerate}[label=\roman*.]
					\item $X_1, \ldots, X_n$ are independent
					\item $p(x_1, \ldots, x_n) = \prob{X_1 = x_1} \cdots \prob{X_n = x_n}$ for every $x_i \in W_i, \ldots, x_n \in W_n$
				\end{enumerate}
			\end{proposition}


		\subsection{Continuous joint distribution}
			% Continuous joint distribution
			\begin{definition}
				Let $n \geq 1$, some r.v.'s $X_n, \ldots, X_n : \Omega \to \mathbb{R}$ have a \mydef{continuous joint distribution} if there exists a function $f : \mathbb{R}^n \to \mathbb{R}_+$ s.t. $\prob{X_1 \leq a_1, \ldots, X_n \leq a_n}$
				$$
				 = \int_{-\infty}^{a_1} \cdots \int_{-\infty}^{a_n} f(x_1, \ldots, x_n)dx_n \cdots dx_1
				$$
				for every $a_1, \ldots, a_n \in \mathbb{R}$. A function $f$ as above is called a \mydef{joint density of $(X,Y)$}.
			\end{definition}

			\begin{intuition}
				$f(x_1, \ldots, x_n)dx_1 \cdots dx_n$ represents the probability that the random vector $(X_1, \ldots, X_n)$ lies in the small region $\left[x_1, x_1 + dx_1\right] \times \ldots \times \left[x_n, x_n + dx_n\right]$.  
			\end{intuition}

			% Expectation of the image
			\begin{proposition}
				(\mydef{Expectation of the image}).
				Let $\phi : \mathbb{R}^n \to \mathbb{R}$. If $X_1, \ldots, X_n$ have joint density $f$, then the expectation of the r.v. $Z = \phi(X_1, \ldots, X_n)$ can be calculated by the formula $\expec{\phi(X_1, \ldots, X_n)}$
				$$
					= \int_{-\infty}^\infty \cdots \int_{-\infty}^\infty \phi(x_1, \ldots, x_n) \cdot f(x_1, \ldots, x_n) dx_n \cdots dx_1
				$$
			\end{proposition}


		\subsection{Marginal densities}
			\begin{proposition}
				Let $X_1, \ldots, X_n$ be $n$ r.v.'s with a joint density $f = f_{X_1, \ldots, X_n}$. Then for every $i$, $X_i$ is a continuous r.v. with density $f_i$ given by $f_i(z)$
				$$
				= \int\limits_{(x_1, \ldots, x_{i-1}, x_{i+1}, \ldots, x_n) \in \mathbb{R}^{n-1}}
						f(x_1, \ldots, x_{i-1}, z, x_{i+1}, \ldots, x_n) 
				$$
				$$
					dx_1 \cdots dx_{i-1} dx_{i+1} \cdots dx_n
				$$
			\end{proposition}

			% Independence for continuous r.v.'s
			\begin{proposition}
				(\mydef{Independence for continuous r.v.'s}).
				Let $X_1, \ldots, X_n$ be $n$ continuous r.v.'s with respective densities $f_1, \ldots, f_n$. The following are equivalent
				\begin{enumerate}[label=\roman*.]
					\item $X_1, \ldots, X_n$ are independent
					\item $X_1, \ldots, X_n$ are jointly continuous with joint density $f(x_1, \ldots, x_n) = f_1(x_1) \cdots f_n(x_n)$
				\end{enumerate}
			\end{proposition}


	\section{Asymptotic results}
		For this section, fix a probability space $(\Omega, \Fb, \mathbb{P}$ and an infinite sequence of i.i.d. r.v.'s $X_1, X_2, \ldots$. For every $n$, consider the partial sum $S_n = X_1 + \ldots + X_n$.
		\begin{definition}
			The r.v. defined by $\frac{S_n}{n} = \frac{X_1 + \ldots + X_n}{n}$ (when $n$ is large) is called the \mydef{empirical average}.
		\end{definition}


		\subsection{Law of large numbers}
			\begin{proposition}
				Assume that $\expec{\lvert X_1 \lvert} < \infty$. Defining $m = \expec{X_1}$ we have 
				$
					\lim\limits_{n \to \infty} \frac{X_1 + \ldots + X_n}{n} = m
				$ a.s.
			\end{proposition}


		\subsection{Monte-Carlo integration}


		\subsection{Convergence in distribution}
			\begin{definition}
				Let $(X_n)_{n \in \mathbb{N}}$ and $X$ be some r.v.'s. We write $X_n \approx X$ as $n \to \infty$, if for every $x \in \mathbb{R}$ : $\lim\limits{n \to \infty} \prob{X_n \leq x} = \prob{X \leq x}$
			\end{definition}

		
		\subsection{Central limit theorem}
			\begin{proposition}
				(\mydef{Central limit theorem}).
			Assume that $\expec{X_1^2}$ is well defined and finite. Defining $m = \expec{X_1}$ and $\sigma^2 = \text{Var}(X_1)$, we have 
				$$
				\prob{\frac{S_n - n\cdot m}{\sqrt{\sigma^2 n}} \leq a} \underset{n \to \infty}{\longrightarrow} \Phi(a) = \frac{1}{\sqrt{2\phi}} \int_{-\infty}^a e^{-x^2 / 2} dx
				$$
				for every $a \in \mathbb{R}$.
			\end{proposition}




% Statistics 
\bigskip
\noindent\rule{\linewidth}{1.5pt} % Thick horizontal line
\begin{center}
	\Large\bfseries Statistics 
\end{center}

\setcounter{section}{0}


	\section{Basic concepts of an estimator}
		We wish to estimate an unknown parameter $\theta$ based on a sample $X_1, X_2, \ldots, X_n$.

		
		% Estimator
		\subsection{Definition of an estimator}
			\begin{definition}
				An \mydef{estimator} is a r.v. $T : \Omega \to \mathbb{R}$ of the form 
				$$
					T = t(X_1, X_2, \ldots, X_n)
				$$
				where $t : \mathbb{R}^n \to \mathbb{R}$ is a measurable function. Inserting the observed data 
				$$
					x_1, x_2, \ldots, x_n \ \text{with }x_i = X_i(\omega)\text{)}
				$$
				yields the \mydef{estimate} $t(x_1, \ldots, x_n)$ for $\theta$.
			\end{definition}

			\begin{definition}
				\begin{enumerate}
					\item \mydef{Last observation estimator} : $T^{(1)} = X_n$
					\item \mydef{Sample mean estimator} : $T^{(2)} = \frac{1}{n} \sum^n_{i = 1} X_i$
				\end{enumerate}
			\end{definition}


		\subsection{Bias and mean squared error}
			The estimator $T$ is a random variable which distribution (under $\mathbb{P}_\theta$) depends on the unknown parameter $\theta$.

			\begin{definition}
				An estimator $T$ is called \mydef{unbiased} for $\theta$ if, for all $\theta \in \Theta$ :
				$$
					\expect{T} = \theta
				$$
			\end{definition}

			\begin{definition}
				For $\theta \in \Theta$, the \mydef{bias} of an estimator $T$ is defined as 
				$$
					\biast{T} = \expect{T} - \theta
				$$
				The \mydef{mean squared error} (MSE) is defined as 
				$$
					\mset{T} = \expect{(T - \theta)^2}
				$$
				For an unbiased estimator, the MSE equals the variance :
				$$
					\mset{T} = \vart{T} + (\biast{T})^2
				$$
			\end{definition}


			\subsection{Maximum likelihood estimation (MLE)}
				\begin{definition}
					For the observed sample $(x_1, \ldots, x_n)$, the \mydef{likelihood} function is defined by
					$$
						L(x_1, \ldots, x_n ; \theta) = 
							\begin{cases}
								\prod^{n}_{i = 1} p_{X_i}(x_i ; \theta), \quad \text{if } X_i \text{ are discrete}\\
								\prod^{n}_{i = 1} f_{X_i}(x_i ; \theta), \quad \text{if } X_i \text{ are continuous}
							\end{cases}
					$$
				\end{definition}

				\begin{definition}
					The \mydef{maximum likelihood estimator} of $\theta$ is defined as 
					$$
						\hat{\theta}(x_1, \ldots, x_n) \in \arg\max_{\theta \in \Theta} L(x_1, \ldots, x_n ; \theta)
					$$
					In practice, one maximises the log-likelihood function $l(\theta ; x_1, \ldots, x_n) = \log L(x_1, \ldots, x_n ; \theta)$, and then obtains the estimator by replacing the data with the random variables : 
					$$
						T_{ML} = t_{ML}(X_1, \ldots, X_n)
					$$
				\end{definition}


				\subsubsection{Application of the method}
					The maximum likelihood method is a way to systematically determine an estimator.
					\begin{enumerate}
						\item Find the joint density / distribution of the random variables
						\item Determine the \textbf{log-likelihood function} from it : $f(\theta) := \ln (L(x_1, \ldots, x_n ; \theta))$
						\item Differentiate $f(\theta)$ with respect to $\theta$
						\item Find the zero(s) of $f'(\theta)$
						\item Show that $f''(\theta) < 0$ or use another argument to demonstrate that a maximum has been found (possibly check boundary points)
					\end{enumerate}


			\subsection{Models with multiple parameters}
				Consider the parameter space $\Theta \subset \mathbb{R}^m$, where $m$ is the number of parameters. The stochastic model is given by a family of probability measures $(P_\theta)_{\theta in \Theta}$, and our goal is to estimate the vector 
				$$
					\theta = (\theta_1, \theta_2, \ldots, \theta_m).
				$$
				All previous definitions extend to this setting.


		\section{Confidence intervals}
			

			\subsection{Definition}
				\begin{definition}
					Let $\alpha \in [0,1]$. A \mydef{confidence interval} for $\theta$ with confidence level $1-\alpha$ is a random interval $I = [A,B]$, with endpoints $A = a(X_1, \ldots, X_n), \ B = b(X_1, \ldots, X_n)$, where $a,b : \mathbb{R}^n \to \mathbb{R}$, s.t. for all $\theta \in \Theta$ 
					$$
						P_\theta[A \leq \theta \leq B] \geq 1 - \alpha
					$$
				\end{definition}


			\subsection{Distribution statements}
				\begin{definition}
					A continuous r.v. $X$ is said to be \mydef{chi-squared distributed} with $m$ degrees of freedom if its density is given by 
					$$
					f_X(y) = \frac{1}{2^{m/2} \Gamma(m/2)} y^{\frac{m}{2} - 1} e^{-y/2}, \quad y \geq 0
					$$
					Where $\Gamma(v) = \int^\infty_0 t^{v-1}e^{-t}dt$ and for $n \in \mathbb{N}, \ \Gamma(n) = (n-1)!$\\
					We write $X \sim \chi^2_m$.
				\end{definition}

				\begin{proposition}
					(\mydef{Sum of squares theorem}). If $X_1, X_2, \ldots, X_m$ are iid $\sim N(0,1)$, then 
					$$
						Y = \sum^m_{i = 1} X^2_i \sim \chi^2_m
					$$
				\end{proposition}

				\begin{definition}
					A continuous r.v. X is said to be \mydef{t-distributed} with $m$ degrees of freedom if its density is given by 
					$$
						f_X(x) = \frac{\Gamma(\frac{m+1}{2})}{\sqrt{m\pi}\Gamma(\frac{m}{2})} \left( 1 + \frac{x^2}{m} \right)^{-\frac{m+1}{2}}, \quad x \in \mathbb{R}
					$$
					We write $X \sim t_m$.
				\end{definition}

				\begin{proposition}
					Let $X$ and $Y$ be independent r.v. with $X \sim N(0,1)$ and $Y \sim \chi^2_m$. Then the quotient
					$$
						Z := \frac{X}{\sqrt{Y/m}}
					$$
					is t-distributed with $m$ degrees of freedom.
				\end{proposition}


			\subsection{Normal model with unknown variance and mean}
				\begin{definition}
					\mydef{Sample mean} : $\bar{X} = \frac{1}{n} \sum^n_{i=1}X_i$ \\
					\mydef{Sample variance} : $S^2 = \frac{1}{n-1} \sum^n_{i+1}(X_i - \bar{X_n})^2$
				\end{definition}

				\begin{proposition}
					If $X_1, \ldots, X_n$ are iid $\sim N(m, \sigma^2)$, then $\bar{X}_n$ and $S^2$ are independent.
				\end{proposition}


			\subsection{Approximate confidence intervals}
				A general approximate approach is provided by the central limit theorem (CLT). Often, an estimator $T$ is a function of a sum, say, $T = \frac{1}{n} \sum^n_{i=1} Y_i$. By the CLT for large $n$,
				$$
					\sum^n_{i = 1} Y_i \approx N\left(n\expec{Y_i}, n  \text{Var}[Y_i]\right)
				$$
				which can be used to approximate the distribution $T$ and hence to construct approximate confidence intervals.


		\section{Tests}
			\subsection{Null and alternative hypotheses}
				Starting with a sample $X_1, \ldots, X_n$, consider a family of probability measures $P_\theta$ with $\theta \in \Theta$ that describes our possible models. The basic problem is to decide between two classes of models - namely, the null hypothesis and the alternative hypothesis. \\
				One sets 
				\begin{itemize}
					\item \mydef{Null hypothesis} $H_0 : \theta \in \Theta_0$
					\item \mydef{Alternative hypothesis} $H_A : \theta \in \Theta_A$
				\end{itemize}
				with $\Theta_0 \cap \Theta_A = \emptyset$ (default : $\Theta_A = \Theta \setminus \Theta_0)$. When $\Theta_0$ or $\Theta_A$ consists of a single value $\theta_0$ or $\theta_A$, they are called \mydef{simple}, otherwise they are called \mydef{composite}.



			\subsection{Tests and decisions}
				\begin{definition}
					A \mydef{test} is a pair $(T, K)$, where 
					\begin{itemize}
						\item $T$ is a statistic of the form $T = t(X_1, \ldots, X_n)$ (the \mydef{test statistic}) and 
						\item $K \subset \mathbb{R}$ is a (deterministic) set, called the \mydef{critical region} (or \mydef{rejection region}).
					\end{itemize}
				\end{definition}

				A statistical test enables us to systematically accept or reject the null hypothesis. We first compute the test statistic $T(\omega) = t(X_1(\omega), \ldots, X_n(\omega))$ and then follow the decision rule : reject $H_0$ if $T(\omega) \in K$ and don't reject $H_0$ if $T(\omega) \notin K$. \\

				There are two types of errors :
				\begin{enumerate}
					\item A \mydef{Type I error} occurs when the null hypothesis is wrongly rejected even though it is true. Probability : $P_\theta[T \in K]$, for $\theta \in \Theta_0$.
					\item A \mydef{Type II error} occurs when the null hypothesis is not rejected even though it is false. Probability : $P_\theta[T \notin K] = 1 - P_\theta[T \in K]$ for $\theta \in \Theta_A$.
				\end{enumerate}


			\subsection{Significance level and power}
				\begin{definition}
					Let $\alpha \in (0,1)$. A test $(T, K)$ is said to have \mydef{significance level} $\alpha$ if for all $\theta \in \Theta_0$
					$$
						P_\theta [T \in K] \leq \alpha
					$$
				\end{definition}
				
				\begin{intuition}
					The significance level is the probability you're willing to accept of making a wrong decision, specifically rejecting the null hypothesis when it's actually true.
				\end{intuition}

				\begin{definition}
					The \mydef{power} of a test $(T, K)$ is the function 
					$$
					\beta : \Theta_A \to [0,1], \quad \theta \mapsto \beta(\theta) := P_\theta[T \in K]
					$$
				\end{definition}
					
				\begin{intuition}
					The power is the "strength" of the test to avoid failing to detect an effect when one actually exists.
				\end{intuition}


			\subsection{Construction of tests}
				Assume $\theta_0 \neq \theta_A$ are two fixed numbers. Assume both the null hypothesis and alternative hypothesis are simple, i.e. $H_0 : \theta = \theta_0 \quad H_A : \theta = \theta_A$. Assume r.v. $X_1, \ldots, X_n$ are either jointly discrete or jointly continuous under both $P_{\theta_0}$ and $P_{\theta_A}$. In particular, $L(x_1, \ldots, x_n ; \theta)$ is well-defined for $\theta = \theta_0$ and $\theta = \theta_A$.

				\begin{definition}
					For every $x_1, \ldots, x_n$, the \mydef{likelihood ratio} is defined by 
					$$
						R(x_1, \ldots, x_n) := \frac{L(x_1, \ldots, x_n ; \theta_A)}{L(x_1, \ldots, x_n ; \theta_0)}
					$$
					By convention, if $L(x_1, \ldots, x_n ; \theta_0) = 0$, we set the ratio to $+\infty$.
				\end{definition}

				\begin{intuition}
					A large ratio indicates that the observations $x_1, \ldots, x_n$ are far more likely under the alternative $P_{\theta_A}$ than under the null $P_{\theta_0}$. Hence it makes sense to define the test statistic as $T := R(X_1, \ldots, X_n)$ and the critical region as $K := (c, \infty)$, for some constant $c$.
				\end{intuition}

				\begin{definition}
					Let $c \geq 0$. The \mydef{likelihood ratio test} with parameter $c$ is the test $(T,K)$ where $T = R(X_1, \ldots, X_n)$ and $K = (c, \infty)$. It is optimal : no test have lower power while no greater significance level (\mydef{Neyman-Pearson Lemma}).
				\end{definition}

				\begin{definition}
					For composite hypotheses, the \mydef{generalized likelihood ratio} can be defined as 
					$$
						R(x_1, \ldots, x_n) := \frac{\sup_{\theta \in \Theta_A} L(x_1, \ldots, x_n ; \theta)}{\sup_{\theta \in \Theta_0} L(x_1, \ldots, x_n ; \theta)}
					$$
					and we choose $T := R(X_1, \ldots, X_n)$ with $K = (c_0, \infty)$ where $c_0$ is chosen s.t. the test has the preassigned significance level.
				\end{definition}


			\subsection{The p-value}
				Let $X_1, \ldots, X_n$ be a sample of size $n$. We wish to test a hypothesis $H_0 : \theta = \theta_0$ against an alternative $H_A : \theta \in \Theta_A$.

				\begin{definition}
					A \mydef{family of tests} $(T, (K_t)_{t \geq 0})$ is said to be \mydef{ordered} with respect to the test statistic $T$ if for all $s,t \geq 0$
					$$
						s \leq t \ \Longrightarrow \ K_s \subset K_t
					$$
					Typical examples are :
					$$
						K_t = (t, \infty) \ \text{(right-tailed test),} \ K_t = (-\infty, -t) \ \text{(left-tailed test),}
					$$
					$$
					 	\text{or } \quad K_t = (-\infty, -t) \cup (t, \infty) \ \text{(two-sided test).}
					$$
				\end{definition}

				\begin{definition}
					Let $H_0 : \theta = \theta_0$ be a simple null hypothesis and let $(T, (K_t)_{t \geq 0})$ be an ordered family of tests. The \mydef{p-value} is defined as the r.v. $\text{p-value} = G(T)$, where the function $G : \mathbb{R}^+ \to [0,1]$ is given by 
					$$
						G(t) = P_\theta [T \in K_t]
					$$
				\end{definition}

				\begin{intuition}
					The p-value informs us which tests in our family would lead to rejection of $H_0$. If the observed p-value is $p$, then every test with significance level $a > p$ would reject $H_0$ and those with $a \leq p$ would not. The p-value doesn't depend on the alternative hypothesis.
				\end{intuition}


\newpage
\squaredpaper
\vfill


	
\end{multicols}	
\end{document}
